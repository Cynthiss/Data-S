import requests
from bs4 import BeautifulSoup

# URL base del listado de propiedades
url_base = "https://mapainmueble.com/apartamentos-en-alquiler-zona-14/"

# Simular un navegador para evitar bloqueos
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# 游늷 FUNCION PARA OBTENER TODAS LAS PROPIEDADES DE TODAS LAS P츼GINAS
def obtener_links_propiedades():
    propiedad_links = []
    pagina_actual = 1

    while True:
        # Construir la URL de la p치gina actual
        url_pagina = f"{url_base}page/{pagina_actual}/" if pagina_actual > 1 else url_base
        print(f"[游댌] Extrayendo propiedades de: {url_pagina}")

        # Hacer la solicitud HTTP
        response = requests.get(url_pagina, headers=headers)

        # Verificar si la p치gina responde correctamente
        if response.status_code != 200:
            print(f"[仇] No se pudo acceder a la p치gina {pagina_actual}. C칩digo: {response.status_code}")
            break

        # Crear objeto BeautifulSoup con el HTML de la p치gina
        soup = BeautifulSoup(response.text, "html.parser")

        # Buscar todos los contenedores de propiedades
        contenedores = soup.find_all("div", class_="col-md-4 has_prop_slider listing_wrapper")
        if not contenedores:
            print(f"[游뛂] No se encontraron propiedades en la p치gina {pagina_actual}. Deteniendo b칰squeda.")
            break

        # Extraer los links desde `data-modal-link`
        for contenedor in contenedores:
            url = contenedor.get("data-modal-link")
            if url:
                propiedad_links.append(url)

        # Buscar la paginaci칩n
        paginacion = soup.find("ul", class_="pagination")
        if not paginacion or not soup.find("a", text=str(pagina_actual + 1)):
            print(f"[九] No hay m치s p치ginas despu칠s de la {pagina_actual}. Finalizando extracci칩n.")
            break

        # Pasar a la siguiente p치gina
        pagina_actual += 1

    print(f"\n[九] Total de propiedades encontradas: {len(propiedad_links)}")
    return propiedad_links

# 游늷 Ejecutar el scraper y obtener los links de todas las propiedades
links = obtener_links_propiedades()

# 游늷 Mostrar los links extra칤dos
print("\n游댳 **Propiedades encontradas:**")
for i, link in enumerate(links, 1):
    print(f"{i}. {link}")
